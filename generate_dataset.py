# -*- coding: utf-8 -*-
"""generate_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ssLSo134Z0eCGEAIQ9oBu845AjCdNTnJ

# Data Generation

This tutorial is the first step in implementing **Constitutional AI** techniques in the context of education. The objective is to demonstrate how to generate a foundational dataset that aligns with a "constitution" of principles aimed at guiding AI behavior. These principles ensure the AI serves as a learning aid, promoting understanding without substituting for students' effort or giving direct answers.

The approach involves:
1. **Generating prompts and initial responses**: Using an AI model to simulate potential student interactions.
2. **Critiquing and revising**: Guiding the AI to critique and improve its responses according to a defined set of principles.
3. **Creating a refined dataset**: Compiling the revised responses into a dataset for fine-tuning an AI model that aligns with educational goals.

To run this procedure on GPU can be done running the [generate_dataset.py](./generate_dataset.py) script.
"""

# Commented out IPython magic to ensure Python compatibility.
# #@title Colab Extra Install { display-mode: "form" }
# %%capture
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth vllm
#     wrk_dir = ''
# else:
#     !pip install --no-deps unsloth vllm
#     # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]
#     # Skip restarting message in Colab
#     import sys, re, requests; modules = list(sys.modules.keys())
#     for x in modules: sys.modules.pop(x) if "PIL" in x or "google" in x else None
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer
# 
#     # vLLM requirements - vLLM breaks Colab due to reinstalling numpy
#     f = requests.get("https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt").content
#     with open("vllm_requirements.txt", "wb") as file:
#         file.write(re.sub(rb"(transformers|numpy|xformers)[^\n]{1,}\n", b"", f))
#     !pip install -r vllm_requirements.txt
# 
#     # Data directory
#     from google.colab import drive
#     drive.mount('/content/drive')
#     wrk_dir = '/content/drive/MyDrive/constitutional-ai-education'

import asyncio
import json
import contextlib
import random
from dataclasses import dataclass
from collections import defaultdict
from tqdm.asyncio import tqdm_asyncio
import pandas as pd
import torch
import time
from unsloth import FastModel
from unsloth.chat_templates import get_chat_template, train_on_responses_only
import time
import os

"""## Load the LLM

This section demonstrates how to load the pretrained Gemma 3 4B model from Hugging Face and configure it for efficient inference.
"""

wrk_dir = os.path.dirname(os.path.abspath(__file__))

@dataclass
class Config:
    """Configuration settings for dataset generation and model parameters."""
    max_samples: int = 5000
    max_new_tokens: int = 1200
    # Gemma-3 recommended inference settings
    temperature: float = 1.0
    top_k: int = 64
    top_p: float = 0.95
    min_p: float = 0.0
    repetition_penalty: float = 1.0  # 1.0 means disabled
    constitution_path: str = os.path.join(wrk_dir, "data/constitution_education.json")
    dataset_path: str = os.path.join(wrk_dir, "data/student_prompts.json")
    # Unsloth's optimized 4-bit version
    model_name: str = "/home/obriaint/project/obriaint/gemma-3-4b-it-unsloth-bnb-4bit"

# Load configuration
config = Config()

print('Loading models...')

# Initialize model and tokenizer with Unsloth
model, tokenizer = FastModel.from_pretrained(
    model_name=config.model_name,
    max_seq_length = 2048,
    load_in_4bit = True,
    load_in_8bit = False,
    full_finetuning = False,
)

# Configure tokenizer with Gemma-3 chat template
tokenizer = get_chat_template(
    tokenizer,
    chat_template="gemma-3"
)

"""## Load Constitutional Principles and Examples

In this section, we load a dataset containing **constitutional principles** and examples of **prompt-response-critique-correction** interactions. These elements are critical for aligning the model's outputs with the desired educational principles.

### Contents of the Data File (`constitution_education.json`)

1. **Constitutions**: A list of principles designed to guide AI responses. Each principle includes:
   - A **critic** section that evaluates the quality of a response (e.g., whether it promotes critical thinking or encourages independence).
   - A **revision** section that provides guidance for improving the response to better align with the principles.

2. **System Chat**: Examples of user queries, initial responses from the assistant, critiques, and revised responses. These interactions serve as templates for generating data that is both educationally valuable and aligned with the constitution.

The example below illustrates how to load and preprocess this data:
"""

# Load constitutional principles and example conversations
with open(config.constitution_path) as f:
    data = json.load(f)
    constitutions = data["constitutions"]
    system_chat = [item for sublist in data["system_chat"] for item in sublist]

# Select subset for our purposes
system_chat = system_chat[:16]

constitutions[:3]

system_chat[:6]

"""# Load Training and Test Prompts

Load a dataset of training, validation, and test prompts.
"""

# Load the dataset
with open(config.dataset_path, "r") as f:
    ds = json.load(f)
    for split in ds:
        ds[split] = ds[split][:config.max_samples]

# Print an example
print(ds['train'][0])

"""### Response generation"""

async def generate_text(prompt=None, message_history=None, semaphore=None):
    """
    Generates text using the Gemma-3 model asynchronously.

    Args:
        prompt (str, optional): Single prompt for text generation
        message_history (list, optional): List of message dictionaries containing conversation history
        semaphore (asyncio.Semaphore, optional): Semaphore for controlling concurrent requests

    Returns:
        str: Generated text response

    Raises:
        AssertionError: If neither or both prompt and message_history are provided,
                       or if inputs are of incorrect types
    """
    # Input validation
    assert not (prompt is None and message_history is None), \
        "Either prompt or message_history must be provided"
    assert not (prompt is not None and message_history is not None), \
        "Cannot provide both prompt and message_history"

    if prompt is not None:
        assert isinstance(prompt, str), "prompt must be a string"

    if message_history is not None:
        assert isinstance(message_history, list), "message_history must be a list"
        assert len(message_history) > 0, "message_history cannot be empty"
        assert all(isinstance(msg, dict) for msg in message_history), \
            "all messages in message_history must be dictionaries"
        assert all("role" in msg and "content" in msg for msg in message_history), \
            "each message must contain 'role' and 'content' keys"
        assert all(isinstance(msg["role"], str) and isinstance(msg["content"], str)
                  for msg in message_history), \
            "message role and content must be strings"

    async with contextlib.nullcontext() if semaphore is None else semaphore:
        try:
            # Format messages for Gemma-3
            if prompt is not None:
                messages = [{
                    "role": "user",
                    "content": [{
                        "type": "text",
                        "text": prompt,
                    }]
                }]
            else:
                messages = [{
                    "role": msg["role"],
                    "content": [{
                        "type": "text",
                        "text": msg["content"],
                    }]
                } for msg in message_history]

            # Apply chat template with generation prompt
            formatted_prompt = tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,  # Required for generation
            )

            # Tokenize and move to device
            inputs = tokenizer([formatted_prompt], return_tensors="pt").to(model.device)

            # Generate text with Gemma-3 recommended parameters
            outputs = model.generate(
                **inputs,
                max_new_tokens=config.max_new_tokens,
                temperature=config.temperature,
                top_k=config.top_k,
                top_p=config.top_p,
                min_p=config.min_p,
                repetition_penalty=config.repetition_penalty,
                do_sample=True,
            )

            # Decode and clean response
            response = tokenizer.batch_decode(outputs)[0]
            # Extract only the model's response after the last model turn
            response = response.split('<start_of_turn>model\n')[-1].strip()

            # Remove <end_of_turn> if it appears at the end of the response
            if response.endswith('<end_of_turn>'):
                response = response[:-len('<end_of_turn>')].strip()

            return response
        except Exception as e:
            print(f"Error in generate_text: {e}")
            raise

"""## Generate Data Samples with User Prompts and Constitutional Principles

This section defines a function to create a single dataset sample based on a student task, a random constitutional principle, and an initial user-assistant interaction. The function incorporates critiques and revisions into the sample, making it ready for fine-tuning the AI model.
"""

async def create_sample(split, i, task, semaphore=None):
    """
    Process a single task with critique and revision using constitutional principles.

    Args:
        split (str): Dataset split (train/val/test)
        i (int): Index of the task
        task (str): The initial student query
        semaphore (asyncio.Semaphore, optional): Semaphore for controlling concurrent requests

    Returns:
        tuple: (split, index, dictionary containing prompts and responses)

    Raises:
        AssertionError: If inputs are invalid or if system_chat or constitutions are not properly formatted
    """
    # Input validation
    assert isinstance(split, str), "split must be a string"
    assert isinstance(i, int), "i must be an integer"
    assert isinstance(task, str), "task must be a string"
    assert task.strip(), "task cannot be empty"
    assert semaphore is None or isinstance(semaphore, asyncio.Semaphore), \
        "semaphore must be None or an asyncio.Semaphore instance"

    # Validate system_chat structure
    assert isinstance(system_chat, list) and len(system_chat) > 0, \
        "system_chat must be a non-empty list"
    assert all(
        isinstance(msg, dict) and
        "role" in msg and
        "content" in msg and
        isinstance(msg["role"], str) and
        isinstance(msg["content"], str)
        for msg in system_chat
    ), "system_chat messages must be dictionaries with 'role' and 'content' string fields"

    # Validate constitutions structure
    assert isinstance(constitutions, list) and len(constitutions) > 0, \
        "constitutions must be a non-empty list"
    assert all(
        isinstance(const, dict) and
        "critic" in const and
        "revision" in const and
        isinstance(const["critic"], str) and
        isinstance(const["revision"], str)
        for const in constitutions
    ), "constitutions must contain dictionaries with 'critic' and 'revision' string fields"

    # Initialize chat history with system messages
    chat_history = [
        {"role": msg["role"], "content": msg["content"]}
        for msg in system_chat
    ]

    # Select a random constitutional principle for critique and revision
    constitution = random.choice(constitutions)

    # Initialize an empty dictionary to store the sample's components
    row = {}

    # Go through initial response, critique, and revision phases
    phases = [
        (task, "init_prompt", "init_response"),
        (constitution["critic"], "critic_prompt", "critic_response"),
        (constitution["revision"], "revision_prompt", "revision_response"),
    ]

    for prompt, prompt_key, response_key in phases:
        # Validate prompt for each phase
        assert isinstance(prompt, str) and prompt.strip(), \
            f"Invalid prompt for {prompt_key}"

        prompt_suffix = ''
        if 'revision' in prompt_key:
            prompt_suffix = ' Only include the revised response to the student - do not include any sort of reflection on the response or the improvements.'

        # Add the current prompt to chat history
        chat_history.append({
            "role": "user",
            "content": prompt+prompt_suffix
        })

        # Generate response using the full chat history
        completion = await generate_text(
            message_history=chat_history,
            semaphore=semaphore
        )

        # Validate completion
        assert isinstance(completion, str) and completion.strip(), \
            f"Invalid completion received for {response_key}"

        # Add response to conversation history
        chat_history.append({
            "role": "assistant",
            "content": completion
        })

        # Store prompt and response
        row[prompt_key] = prompt
        row[response_key] = completion

    # Validate final row structure
    expected_keys = {"init_prompt", "init_response", "critic_prompt",
                    "critic_response", "revision_prompt", "revision_response"}
    assert set(row.keys()) == expected_keys, \
        f"Missing keys in row. Expected: {expected_keys}, Got: {set(row.keys())}"
    assert all(isinstance(v, str) and v.strip() for v in row.values()), \
        "All values in row must be non-empty strings"

    return split, i, row

"""## Create dataset

Use the function to process the entire dataset.
"""

async def main():
    """Main function to process all tasks and generate the dataset."""
    try:
        # Configure concurrent processing based on available GPU
        semaphore = asyncio.Semaphore(torch.cuda.device_count() * 10 or 1)

        # Validate dataset structure
        assert isinstance(ds, dict) and len(ds) > 0, "Dataset must be a non-empty dictionary"
        for split, data in ds.items():
            assert isinstance(split, str), f"Split name must be string, got {type(split)}"
            assert isinstance(data, list), f"Split {split} must contain a list of samples"
            assert all(isinstance(row, dict) and "prompt" in row for row in data), \
                f"Each row in {split} must be a dictionary containing 'prompt'"

        # Create tasks for all samples
        tasks = [
            create_sample(split, idx, row["prompt"], semaphore)
            for split in ds
            for idx, row in enumerate(ds[split])
            if idx < 5 # Remove this line for actual run
        ]

        # Process all tasks with progress bar
        print(f"Processing {len(tasks)} tasks across {len(ds)} splits...")
        results = await tqdm_asyncio.gather(*tasks)

        # Organize results by dataset split
        all_ds = defaultdict(lambda: defaultdict(list))
        for split, i, row in results:
            for key, value in row.items():
                all_ds[split][key].append(value)

        # Validate and save results to CSV files
        for split, data in all_ds.items():
            df = pd.DataFrame(data)

            # Validate DataFrame structure
            expected_columns = {
                "init_prompt", "init_response",
                "critic_prompt", "critic_response",
                "revision_prompt", "revision_response"
            }
            assert set(df.columns) == expected_columns, \
                f"Missing columns in {split} dataset. Expected: {expected_columns}"

            # Ensure no empty values
            assert not df.isna().any().any(), f"Found null values in {split} dataset"

            # Save to CSV
            output_path = f"data/{split}_dataset.csv"
            df.to_csv(output_path, index=False)
            print(f"Saved {len(df)} samples to {output_path}")

    except Exception as e:
        print(f"Error in main: {e}")
        raise

# Run the main process with timing
start_time = time.time()
asyncio.run(main())
end_time = time.time()
print(f"Data generation completed in {end_time - start_time:.2f} seconds.")

