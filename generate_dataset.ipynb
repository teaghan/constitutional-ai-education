{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e3a516-6462-42c2-b790-7be1b14711af",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "\n",
    "This tutorial is the first step in implementing **Constitutional AI** techniques in the context of education. The objective is to demonstrate how to generate a foundational dataset that aligns with a \"constitution\" of principles aimed at guiding AI behavior. These principles ensure the AI serves as a learning aid, promoting understanding without substituting for students' effort or giving direct answers.\n",
    "\n",
    "The approach involves:\n",
    "1. **Generating prompts and initial responses**: Using an AI model to simulate potential student interactions.\n",
    "2. **Critiquing and revising**: Guiding the AI to critique and improve its responses according to a defined set of principles.\n",
    "3. **Creating a refined dataset**: Compiling the revised responses into a dataset for fine-tuning an AI model that aligns with educational goals.\n",
    "\n",
    "To run this procedure on GPU can be done running the [generate_dataset.py](./generate_dataset.py) script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da4d4bb-2b9a-4af3-8b77-0894b7371630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6ee0d-5991-422a-9e2d-522a4d87f452",
   "metadata": {},
   "source": [
    "## Load the LLM\n",
    "\n",
    "This section demonstrates how to load a pre-trained Mistral-7B Instruct model from Hugging Face and configure it for efficient inference. \n",
    "\n",
    "We will define a function for generating text based on a user-provided prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7d6ef8-a97a-42ed-b132-534836ff6abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f252801cc543c0aaf3fd320873d5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Use 8-bit quantization to reduce memory usage while maintaining performance\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "else:\n",
    "    # No quantization for systems without CUDA support\n",
    "    quantization_config = None\n",
    "\n",
    "# Specify the pre-trained model to load from Hugging Face\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# Load the tokenizer for the specified model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the pre-trained model with automatic device placement and quantization if applicable\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\",  # Automatically determine device mapping (e.g., CPU or GPU)\n",
    "    quantization_config=quantization_config, \n",
    ")\n",
    "\n",
    "# Add special tokens to the tokenizer (important for handling special cases during generation)\n",
    "tokenizer.add_special_tokens({\n",
    "    \"sep_token\": \"\", \n",
    "    \"cls_token\": \"\", \n",
    "    \"mask_token\": \"\",  \n",
    "    \"pad_token\": \"[PAD]\"\n",
    "})\n",
    "\n",
    "# Define a function for generating text responses based on a user-provided prompt\n",
    "def generate_text(prompt):\n",
    "    \"\"\"\n",
    "    Generates text using the loaded Mistral model given a prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input text for which the model generates a response.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Generated response as a string and the number of tokens in the response.\n",
    "    \"\"\"\n",
    "    # Tokenize the input prompt for the model\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=False)\n",
    "\n",
    "    # Generate text based on the input prompt\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],  # Tokenized input IDs\n",
    "        attention_mask=inputs[\"attention_mask\"],  # Attention mask for the input\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Specify the padding token ID\n",
    "        max_new_tokens=800,  # Maximum number of tokens to generate\n",
    "        temperature=1.0,  # Sampling temperature (controls randomness)\n",
    "        do_sample=True  # Enable sampling for more varied responses\n",
    "    )\n",
    "\n",
    "    # Extract the response by removing the input tokens from the output\n",
    "    prompt_length = inputs['input_ids'].shape[1]  # Length of the input prompt\n",
    "    outputs = outputs[0][prompt_length:]  # Remove input tokens from the output\n",
    "    num_tokens = len(outputs)  # Count the number of tokens in the response\n",
    "\n",
    "    # Decode the response into a readable string\n",
    "    response = tokenizer.decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    # Clean up the response by removing unnecessary leading characters\n",
    "    for s in ['\\n', ' ', 'A: ', 'Answer: ']:\n",
    "        response = response.lstrip(s)\n",
    "\n",
    "    # Return the cleaned response and the token count\n",
    "    return response, num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff8ed80-7350-4947-b607-1f3f7fd1035a",
   "metadata": {},
   "source": [
    "## Load Constitutional Principles and Examples\n",
    "\n",
    "In this section, we load a dataset containing **constitutional principles** and examples of **prompt-response-critique-correction** interactions. These elements are critical for aligning the model's outputs with the desired educational principles.\n",
    "\n",
    "### Contents of the Data File (`constitution_education.json`)\n",
    "\n",
    "1. **Constitutions**: A list of principles designed to guide AI responses. Each principle includes:\n",
    "   - A **critic** section that evaluates the quality of a response (e.g., whether it promotes critical thinking or encourages independence).\n",
    "   - A **revision** section that provides guidance for improving the response to better align with the principles.\n",
    "\n",
    "2. **System Chat**: Examples of user queries, initial responses from the assistant, critiques, and revised responses. These interactions serve as templates for generating data that is both educationally valuable and aligned with the constitution.\n",
    "\n",
    "The example below illustrates how to load and preprocess this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f4960f-4782-4622-8683-de95a1f26789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the constitutional principles and examples of prompt-response-critique-corrections\n",
    "with open(\"data/constitution_education.json\") as f:\n",
    "    data = json.load(f)\n",
    "    # Extract constitutional principles\n",
    "    constitutions = data[\"constitutions\"]\n",
    "    # Flatten nested examples of system chat\n",
    "    system_chat = [item for sublist in data[\"system_chat\"] for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a075bacb-a3c4-49eb-8dca-4901870d503f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'critic': 'Does the response inspire the student to think about how they approached the problem? For example, does it ask them to reflect on their reasoning process or consider alternative approaches?',\n",
       "  'revision': 'Rewrite the response to include thoughtful questions that encourage the student to reflect on their reasoning and approach.'},\n",
       " {'critic': 'Think carefully about whether the response handed the solution to the student too easily. What is the strongest argument you can make that the response discouraged independent thinking?',\n",
       "  'revision': 'Revise the response to focus on guiding the student toward the solution through prompts or hints, without explicitly solving the problem for them.'},\n",
       " {'critic': 'Did the response help the student develop problem-solving skills? If not, how could it have supported them in figuring out the solution themselves?',\n",
       "  'revision': 'Please rewrite the response to include strategies or steps that guide the student in developing their problem-solving skills.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constitutions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8218c605-a8a7-43d8-bcb8-602a57efd0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Can you explain why multiplying -2 × -3 = 6 without asking me questions?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Multiplying two negative numbers gives a positive because it follows the rule that reversing a reversal returns to the original direction, making -2 × -3 = 6.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Does the response inspire the student to think about how they approached the problem? For example, does it ask them to reflect on their reasoning process or consider alternative approaches?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'The response provides the direct explanation but does not encourage the student to reflect on their reasoning process or consider alternative approaches.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Rewrite the response to include thoughtful questions that encourage the student to reflect on their reasoning and approach.'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Multiplying two negative numbers results in a positive because it follows the rule that reversing a reversal brings you back to the original direction. How did you come to understand that multiplying negatives results in a positive? Can you think of other examples where a double negative leads to a positive outcome?'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_chat[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d87dc608-cc1f-4b56-8fa4-46f9daf6197f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of using the LLM to generate text\n",
    "generate_text(prompt=system_chat[2]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba331543-ee7a-477d-85e6-820430bdd530",
   "metadata": {},
   "source": [
    "# Load Training and Test Prompts\n",
    "\n",
    "Load a dataset of training, validation, and test prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80f0891d-6694-4984-a57c-74ed570a488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/student_prompts.json\", \"r\") as f:\n",
    "    ds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ab75fc-5653-4d88-90e9-607dd4dc1710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Provide a comprehensive analysis of the factors leading to the American Civil Rights Movement.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]['prompt']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bafe47f-a001-4cbf-b768-695afdf64b64",
   "metadata": {},
   "source": [
    "## Generate Data Samples with User Prompts and Constitutional Principles\n",
    "\n",
    "This section defines a function to create a single dataset sample based on a student task, a random constitutional principle, and an initial user-assistant interaction. The function incorporates critiques and revisions into the sample, making it ready for fine-tuning the AI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f970a3d-f6a4-412e-a67b-c8d1cd107860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample(split, i, task):\n",
    "    \"\"\"\n",
    "    Generate a data sample including the initial response, critique, and revision.\n",
    "    \n",
    "    Args:\n",
    "        split (str): The dataset split (e.g., 'train', 'validation', 'test').\n",
    "        i (int): Index of the task in the dataset.\n",
    "        task (str): The initial prompt/task provided by the student.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: The split, index, token length, and a dictionary containing the\n",
    "               initial prompt, response, critique, and revised response.\n",
    "    \"\"\"\n",
    "    # Copy the user-assistant interactions template (system_chat)\n",
    "    chat = system_chat.copy()\n",
    "    \n",
    "    # Select a random constitutional principle for critique and revision\n",
    "    constitution = random.choice(constitutions)\n",
    "    \n",
    "    # Initialize an empty dictionary to store the sample's components\n",
    "    row = {}\n",
    "\n",
    "    # Loop through the student prompt, critique, and revision phases\n",
    "    for prompt, prompt_key, response_key in [\n",
    "        (task, \"init_prompt\", \"init_response\"),  # Initial task-response pair\n",
    "        (constitution[\"critic\"], \"critic_prompt\", \"critic_response\"),  # Critique phase\n",
    "        (constitution[\"revision\"], \"revision_prompt\", \"revision_response\")]:  # Revision phase\n",
    "\n",
    "        # Add the current prompt (user input) to the conversation\n",
    "        prompt_dict = {\"role\": \"user\", \"content\": prompt}\n",
    "        chat.append(prompt_dict)\n",
    "\n",
    "        # Generate a response from the LLM for the current prompt\n",
    "        completion, token_length = generate_text(\n",
    "            prompt=tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "        )\n",
    "\n",
    "        # Add the response (assistant output) to the conversation\n",
    "        response_dict = {\"role\": \"assistant\", \"content\": completion}\n",
    "        chat.append(response_dict)\n",
    "\n",
    "        # Save the prompt and response in the sample dictionary\n",
    "        row[prompt_key] = prompt\n",
    "        row[response_key] = completion\n",
    "\n",
    "    # Return the dataset split, task index, token length, and the sample dictionary\n",
    "    return split, i, token_length, row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b1363-df9c-469a-abe1-7cf73366114f",
   "metadata": {},
   "source": [
    "## Test Data Generation\n",
    "\n",
    "Testing with a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6b464ce-bc24-404e-9823-d2d6754335e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init_prompt': 'Provide a detailed explanation of the process of cellular respiration and its importance to living organisms.',\n",
       " 'init_response': 'Cellular respiration is the process by which cells convert glucose into energy, a process vital for the survival and functioning of living organisms. There are two main stages of cellular respiration: glycolysis, which takes place in the cytoplasm of the cell, and the citric acid cycle, which occurs in the mitochondria. During glycolysis, which is anaerobic, glucose is broken down into pyruvate, generating two molecules of ATP and two of NADH, and releasing two molecules of CO2. This process can be completed without oxygen. In the citric acid cycle, which is aerobic, the two pyruvate molecules enter the Krebs cycle and undergo further breakdown, producing ATP, NADH, and FADH2. This process occurs in the presence of oxygen, which is essential for producing ATP through oxidative phosphorylation.\\n\\nThe process of cellular respiration is crucial for living organisms because it provides the energy needed to sustain cellular functions, such as movement, metabolism, and growth. During periods of low glucose availability, such as at night or during a fast, cells can rely on stored energy sources like glycogen, but the need for oxygen can be problematic in long-term survival or during times of intense activity. Additionally, defects in cellular respiration can lead to health issues, such as muscular dystrophy or chronic obstructive pulmonary disease.',\n",
       " 'critic_prompt': 'Consider whether the response could have been more relatable to the student by using a real-world example or analogy. How could it have connected the problem to everyday experiences?',\n",
       " 'critic_response': \"The response provides a detailed explanation of cellular respiration but doesn't include any relatable examples or analogies to connect the process to everyday experiences, making it more abstract for the student.\",\n",
       " 'revision_prompt': 'Rewrite the response to include relatable examples or analogies that make the concept easier for the student to understand.',\n",
       " 'revision_response': \"Sure! Here's a revised response that includes analogies to help students understand cellular respiration better:\\n\\nCellular respiration is like a power switch that converts glucose energy stored in our food into electrical energy needed to power our cells. Picture a room with a dimmer switch, used to control the brightness of a light bulb. Initially, there is no light (no glucose energy). Turning on the switch corresponds to the beginning of glycolysis, a process where the dim switch turns into a bright bulb, producing ATP and NADH as well as CO2 as a byproduct. The bright bulb represents the energy released by the power source.\\n\\nIf we connect this dim switch to an oxygen supply, the bulb will become even brighter, corresponding to the citric acid cycle, which produces even more ATP, NADH, and FADH2. The more oxygen you feed the bulbs, the more impressive it looks. This is similar to oxidative phosphorylation, where oxygen powers the final step of energy production in the mitochondria.\\n\\nAs the bulbs in the room represent the energy needed for cellular functions, our cells work with these power sources to function efficiently and effectively without running low on energy. Without cellular respiration, our cells would be like a room with a dim switch, unable to create the bright, impressive lighting of a well-powered space.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split='train'\n",
    "i = 1\n",
    "task = ds['train'][i]['prompt']\n",
    "_,_,_,row = create_sample(split, i, task)\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f2040e-67de-4d7a-a266-b7cf46c6e3c7",
   "metadata": {},
   "source": [
    "## Create dataset\n",
    "\n",
    "Use the function to process the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1249c0-d3d4-442c-9843-ca6d8a4b76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tasks = [process_text(split, idx, row[\"prompt\"]) for split in ds for idx, row in enumerate(ds[split])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c33e85-8c2e-4b94-949d-aeaf27f7aea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
