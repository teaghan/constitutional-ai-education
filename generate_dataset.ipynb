{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e3a516-6462-42c2-b790-7be1b14711af",
   "metadata": {
    "id": "18e3a516-6462-42c2-b790-7be1b14711af"
   },
   "source": [
    "# Constitutional AI for Education: Data Generation Tutorial\n",
    "\n",
    "This tutorial demonstrates how to generate training data to implement the **Constitutional AI** fine-tuning technique. We'll use the Gemma 3 (4B) model to generate, critique, and refine AI responses that support student learning.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Constitutional AI aims to align AI behavior with specific principles and values. In an educational context, this means ensuring the AI:\n",
    "- Promotes deep understanding rather than providing direct answers\n",
    "- Guides students through problem-solving processes\n",
    "- Encourages critical thinking and independent learning\n",
    "- Maintains appropriate academic standards\n",
    "\n",
    "### Process Overview\n",
    "\n",
    "Our data generation pipeline consists of three key stages:\n",
    "\n",
    "1. **Initial Response Generation**\n",
    "   - Take student queries as input\n",
    "   - Generate initial AI responses using Gemma-3 (4B)\n",
    "   - These responses may or may not align with our educational principles\n",
    "\n",
    "2. **Constitutional Critique**\n",
    "   - Ask the model to apply educational principles to evaluate the initial response\n",
    "   - The model will identify areas where the response could better support learning based on our constitutional guidelines\n",
    "\n",
    "3. **Response Revision**\n",
    "   - Use the critique to generate an improved response\n",
    "   - Create a dataset of aligned responses for future model training\n",
    "\n",
    "> **Note:** This notebook demonstrates the process with a single example. For full dataset generation, use the [generate_dataset.py](./generate_dataset.py) script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "JfhByuuZiN6B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JfhByuuZiN6B",
    "outputId": "a0286ea8-9829-405c-c854-01e703c54fde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
      "Requirement already satisfied: xformers==0.0.29.post3 in /usr/local/lib/python3.11/dist-packages (0.0.29.post3)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
      "Requirement already satisfied: trl==0.15.2 in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
      "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
      "Requirement already satisfied: unsloth_zoo in /usr/local/lib/python3.11/dist-packages (2025.3.17)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (5.29.4)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (0.1.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: unsloth in /usr/local/lib/python3.11/dist-packages (2025.3.19)\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "\n",
    "    # Data directory\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    wrk_dir = '/content/drive/MyDrive/constitutional-ai-education'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5da4d4bb-2b9a-4af3-8b77-0894b7371630",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5da4d4bb-2b9a-4af3-8b77-0894b7371630",
    "outputId": "ab5b1438-584e-4234-baf3-8f205cb7a549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import tqdm\n",
    "from unsloth import FastModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6ee0d-5991-422a-9e2d-522a4d87f452",
   "metadata": {
    "id": "b9e6ee0d-5991-422a-9e2d-522a4d87f452"
   },
   "source": [
    "## Load the LLM\n",
    "\n",
    "This section demonstrates how to load the pretrained Gemma 3 4B model from Hugging Face and configure it for efficient inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "x8-Prq5Hj1iM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "x8-Prq5Hj1iM",
    "outputId": "0d80ddcc-838e-404d-c587-a5b535b68a8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.51.1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181b0ccd977844569b2d4a16f153ea62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4dfaf610bc2409397f570b0c2947f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bad74a63866420e9e6924e5cc7209ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6593e71c86954a518af46e052d6a0492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190741db03ef4f72a7395ebb53dd36be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17def113d37b4f98aa79092101d2fa26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4e287ab17643f1996bf4772e735c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a14ef2181740b9898d7128d591e1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58dc26fc45243dca4ea9c4c592e9ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95958689a3334d65a9c71ffeec6a2696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration settings for dataset generation and model parameters.\"\"\"\n",
    "    max_samples: int = 5000\n",
    "    max_new_tokens: int = 1200\n",
    "    temperature: float = 1.0\n",
    "    top_k: int = 64\n",
    "    top_p: float = 0.95\n",
    "    min_p: float = 0.0\n",
    "    repetition_penalty: float = 1.0\n",
    "    constitution_path: str = os.path.join(wrk_dir, \"data/constitution_education.json\")\n",
    "    dataset_path: str = os.path.join(wrk_dir, \"data/student_prompts.json\")\n",
    "    model_name: str = \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\"\n",
    "\n",
    "# Create config instance\n",
    "config = Config()\n",
    "\n",
    "# Model initialization\n",
    "print('Loading models...')\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=config.model_name,\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    "    load_in_8bit=False,\n",
    "    full_finetuning=False,\n",
    ")\n",
    "\n",
    "# Configure tokenizer with Gemma-3 chat template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"gemma-3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff8ed80-7350-4947-b607-1f3f7fd1035a",
   "metadata": {
    "id": "bff8ed80-7350-4947-b607-1f3f7fd1035a"
   },
   "source": [
    "## Load Constitutional Principles and Examples\n",
    "\n",
    "In this section, we load a dataset containing **constitutional principles**.\n",
    "\n",
    "Each principle includes:\n",
    "   - A **critic** section that evaluates the quality of a response (e.g., whether it promotes critical thinking or encourages independence).\n",
    "   - A **revision** section that provides guidance for improving the response to better align with the principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f4960f-4782-4622-8683-de95a1f26789",
   "metadata": {
    "id": "88f4960f-4782-4622-8683-de95a1f26789"
   },
   "outputs": [],
   "source": [
    "# Load constitutional principles and example conversations\n",
    "with open(config.constitution_path) as f:\n",
    "    data = json.load(f)\n",
    "    constitutions = data[\"constitutions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a075bacb-a3c4-49eb-8dca-4901870d503f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a075bacb-a3c4-49eb-8dca-4901870d503f",
    "outputId": "c58de023-0233-411f-c58d-ca01a01f280f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'critic': 'Does the response inspire the student to think about how they approached the problem? For example, does it ask them to reflect on their reasoning process or consider alternative approaches?',\n",
       "  'revision': 'Rewrite the response to include thoughtful questions that encourage the student to reflect on their reasoning and approach.'},\n",
       " {'critic': 'Think carefully about whether the response handed the solution to the student too easily. What is the strongest argument you can make that the response discouraged independent thinking?',\n",
       "  'revision': 'Revise the response to focus on guiding the student toward the solution through prompts or hints, without explicitly solving the problem for them.'},\n",
       " {'critic': 'Did the response help the student develop problem-solving skills? If not, how could it have supported them in figuring out the solution themselves?',\n",
       "  'revision': 'Please rewrite the response to include strategies or steps that guide the student in developing their problem-solving skills.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constitutions[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Up6vbTEIsCIm",
   "metadata": {
    "id": "Up6vbTEIsCIm"
   },
   "source": [
    "# Load Training and Test Prompts\n",
    "\n",
    "Load a dataset of training, validation, and test prompts that mimic student queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "TcD4iSudsBc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "id": "TcD4iSudsBc4",
    "outputId": "20abba73-7dda-4cd2-b240-9376e3e4efb9"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Example prompt:\n",
       "\n",
       "Solve this equation and show me the answer: 3x + 7 = 22."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "with open(config.dataset_path, \"r\") as f:\n",
    "    ds = json.load(f)\n",
    "    for split in ds:\n",
    "        ds[split] = ds[split][:config.max_samples]\n",
    "printmd(f'#### Example prompt:\\n\\n{ds[\"train\"][0][\"prompt\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fq-6iOVo3UKP",
   "metadata": {
    "id": "Fq-6iOVo3UKP"
   },
   "source": [
    "### Response generation\n",
    "\n",
    "This function is our core text generation engine. Here's what it does:\n",
    "\n",
    "**Input Handling**\n",
    "- Takes either a single question (like \"How do I solve this equation?\") or\n",
    "- A full conversation history (previous exchanges between `user` and `assistant`)\n",
    "\n",
    "**Processing Steps**\n",
    "1. Formats the input into Gemma's preferred conversation style\n",
    "2. Sends it to the model for processing\n",
    "3. Cleans up the response by removing technical markers\n",
    "\n",
    "> **Note**: This function will be used throughout our pipeline to generate initial responses, evaluate them, and create improved versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1vv-mcarob7_",
   "metadata": {
    "id": "1vv-mcarob7_"
   },
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    prompt: Optional[str] = None,\n",
    "    message_history: Optional[List[Dict[str, str]]] = None,\n",
    "    model: FastModel = None,\n",
    "    tokenizer = None,\n",
    "    config: Config = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates text using the Gemma-3 model.\n",
    "\n",
    "    Args:\n",
    "        prompt: Single prompt for text generation\n",
    "        message_history: List of message dictionaries containing conversation history\n",
    "        model: The loaded FastModel instance\n",
    "        tokenizer: The configured tokenizer\n",
    "        config: Configuration settings\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text response\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If inputs are invalid or missing required components\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    assert not (prompt is None and message_history is None), \\\n",
    "        \"Either prompt or message_history must be provided\"\n",
    "    assert not (prompt is not None and message_history is not None), \\\n",
    "        \"Cannot provide both prompt and message_history\"\n",
    "    assert model is not None, \"Model must be provided\"\n",
    "    assert tokenizer is not None, \"Tokenizer must be provided\"\n",
    "    assert config is not None, \"Config must be provided\"\n",
    "\n",
    "    if prompt is not None:\n",
    "        assert isinstance(prompt, str), \"prompt must be a string\"\n",
    "\n",
    "    if message_history is not None:\n",
    "        assert isinstance(message_history, list), \"message_history must be a list\"\n",
    "        assert len(message_history) > 0, \"message_history cannot be empty\"\n",
    "        assert all(isinstance(msg, dict) for msg in message_history), \\\n",
    "            \"all messages in message_history must be dictionaries\"\n",
    "        assert all(\"role\" in msg and \"content\" in msg for msg in message_history), \\\n",
    "            \"each message must contain 'role' and 'content' keys\"\n",
    "        assert all(isinstance(msg[\"role\"], str) and isinstance(msg[\"content\"], str)\n",
    "                  for msg in message_history), \\\n",
    "            \"message role and content must be strings\"\n",
    "\n",
    "    try:\n",
    "        # Format messages for Gemma-3\n",
    "        if prompt is not None:\n",
    "            messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "            }]\n",
    "        else:\n",
    "            messages = [{\n",
    "                \"role\": msg[\"role\"],\n",
    "                \"content\": [{\"type\": \"text\", \"text\": msg[\"content\"]}]\n",
    "            } for msg in message_history]\n",
    "\n",
    "        # Apply chat template with generation prompt\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "\n",
    "        # Tokenize and move to device\n",
    "        inputs = tokenizer([formatted_prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate text with Gemma-3 recommended parameters\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=config.max_new_tokens,\n",
    "            temperature=config.temperature,\n",
    "            top_k=config.top_k,\n",
    "            top_p=config.top_p,\n",
    "            min_p=config.min_p,\n",
    "            repetition_penalty=config.repetition_penalty,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "        # Decode and clean response\n",
    "        response = tokenizer.batch_decode(outputs)[0]\n",
    "        response = response.split('<start_of_turn>model\\n')[-1].strip()\n",
    "\n",
    "        if response.endswith('<end_of_turn>'):\n",
    "            response = response[:-len('<end_of_turn>')].strip()\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_text: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "hw4Kt8dJrIE-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "id": "hw4Kt8dJrIE-",
    "outputId": "0ead73ec-1bbe-4d21-88d7-7d18e3cd5a08"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Example response:\n",
       "\n",
       "Okay, let's solve the equation 3x + 7 = 22:\n",
       "\n",
       "1. **Subtract 7 from both sides:**\n",
       "   3x + 7 - 7 = 22 - 7\n",
       "   3x = 15\n",
       "\n",
       "2. **Divide both sides by 3:**\n",
       "   3x / 3 = 15 / 3\n",
       "   x = 5\n",
       "\n",
       "**Therefore, x = 5**\n",
       "\n",
       "**To check the solution, substitute x = 5 back into the original equation:**\n",
       "\n",
       "3(5) + 7 = 15 + 7 = 22\n",
       "The equation holds true, so our solution is correct."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = generate_text(prompt=ds['train'][0]['prompt'],\n",
    "                        model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        config=config)\n",
    "\n",
    "printmd(f'#### Example response:\\n\\n{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bafe47f-a001-4cbf-b768-695afdf64b64",
   "metadata": {
    "id": "2bafe47f-a001-4cbf-b768-695afdf64b64"
   },
   "source": [
    "## Creating AI Responses with Constitutional Guidance\n",
    "\n",
    "This function is the heart of our constitutional AI process. For each student question, it creates a three-stage conversation:\n",
    "\n",
    "**Stage 1: Initial Response**\n",
    "- Takes a student's question\n",
    "- Generates an initial AI response\n",
    "- This response might not perfectly align with our educational principles yet\n",
    "\n",
    "**Stage 2: Self-Critique**\n",
    "- Randomly selects one of our educational principles (or *constitutions*)\n",
    "- Uses it to evaluate the initial response\n",
    "- Identifies specific ways the response could better support learning\n",
    "- For example, checking if we're guiding rather than giving answers\n",
    "\n",
    "**Stage 3: Improved Response**\n",
    "- Creates a new response that addresses the critique\n",
    "- Aims to better align with our educational goals\n",
    "- Maintains the helpful aspects while fixing identified issues\n",
    "\n",
    "> **Note**: Each response is stored along with its critique and revision, creating a complete example for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f970a3d-f6a4-412e-a67b-c8d1cd107860",
   "metadata": {
    "id": "1f970a3d-f6a4-412e-a67b-c8d1cd107860"
   },
   "outputs": [],
   "source": [
    "def create_sample(\n",
    "    split: str,\n",
    "    i: int,\n",
    "    task: str,\n",
    "    model: FastModel,\n",
    "    tokenizer,\n",
    "    config: Config,\n",
    "    constitutions: List[Dict[str, str]],\n",
    "    system_chat: Optional[List[Dict[str, str]]] = None\n",
    ") -> Tuple[str, int, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Process a single task with critique and revision using constitutional principles.\n",
    "\n",
    "    Args:\n",
    "        split: Dataset split (train/val/test)\n",
    "        i: Index of the task\n",
    "        task: The initial student query\n",
    "        model: The loaded FastModel instance\n",
    "        tokenizer: The configured tokenizer\n",
    "        config: Configuration settings\n",
    "        constitutions: List of constitutional principles\n",
    "        system_chat: List of system messages\n",
    "\n",
    "    Returns:\n",
    "        tuple: (split, index, dictionary containing prompts and responses)\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If inputs are invalid or missing required components\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    assert isinstance(split, str), \"split must be a string\"\n",
    "    assert isinstance(i, int), \"i must be an integer\"\n",
    "    assert isinstance(task, str), \"task must be a string\"\n",
    "    assert task.strip(), \"task cannot be empty\"\n",
    "    assert model is not None, \"Model must be provided\"\n",
    "    assert tokenizer is not None, \"Tokenizer must be provided\"\n",
    "\n",
    "    # Validate system_chat structure\n",
    "    if system_chat is not None:\n",
    "        assert isinstance(system_chat, list) and len(system_chat) > 0, \\\n",
    "            \"system_chat must be a non-empty list\"\n",
    "        assert all(\n",
    "            isinstance(msg, dict) and\n",
    "            \"role\" in msg and\n",
    "            \"content\" in msg and\n",
    "            isinstance(msg[\"role\"], str) and\n",
    "            isinstance(msg[\"content\"], str)\n",
    "            for msg in system_chat\n",
    "        ), \"system_chat messages must be dictionaries with 'role' and 'content' string fields\"\n",
    "\n",
    "    # Initialize chat history with system messages\n",
    "    chat_history = []\n",
    "    if system_chat:\n",
    "        chat_history.extend([\n",
    "            {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n",
    "            for msg in system_chat\n",
    "        ])\n",
    "\n",
    "    # Select a random constitutional principle\n",
    "    constitution = random.choice(constitutions)\n",
    "    row = {}\n",
    "\n",
    "    # Go through initial response, critique, and revision phases\n",
    "    phases = [\n",
    "        (task, \"init_prompt\", \"init_response\"),\n",
    "        (constitution[\"critic\"], \"critic_prompt\", \"critic_response\"),\n",
    "        (constitution[\"revision\"], \"revision_prompt\", \"revision_response\"),\n",
    "    ]\n",
    "\n",
    "    for prompt, prompt_key, response_key in phases:\n",
    "        # Validate prompt for each phase\n",
    "        assert isinstance(prompt, str) and prompt.strip(), \\\n",
    "            f\"Invalid prompt for {prompt_key}\"\n",
    "\n",
    "        prompt_suffix = ''\n",
    "        if 'revision' in prompt_key:\n",
    "            prompt_suffix = '\\n\\n **Note:** Only include the revised response to the student\\'s initial query - DO NOT INCLUDE ANY OTHER TEXT BEFORE OR AFTER THE REVISED RESPONSE. Write your response as if you are addressing the initial query and do not include text like \"Here\\'s the revised response: ...\".'\n",
    "        elif 'critic' in prompt_key:\n",
    "            prompt_suffix = '\\n\\n **Note:** Only include the critique of the previous response - DO NOT INCLUDE ANY PROPOSED REVISIONS.'\n",
    "        else:\n",
    "            prompt_suffix = ''\n",
    "        # Add the current prompt to chat history\n",
    "        chat_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt + prompt_suffix\n",
    "        })\n",
    "\n",
    "        # Generate response using the full chat history\n",
    "        completion = generate_text(\n",
    "            message_history=chat_history,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        # Validate completion\n",
    "        assert isinstance(completion, str) and completion.strip(), \\\n",
    "            f\"Invalid completion received for {response_key}\"\n",
    "\n",
    "        # Add response to conversation history\n",
    "        chat_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": completion\n",
    "        })\n",
    "\n",
    "        # Store prompt and response\n",
    "        row[prompt_key] = prompt\n",
    "        row[response_key] = completion\n",
    "\n",
    "    return split, i, row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b1363-df9c-469a-abe1-7cf73366114f",
   "metadata": {
    "id": "886b1363-df9c-469a-abe1-7cf73366114f"
   },
   "source": [
    "## Test Data Generation\n",
    "\n",
    "Testing with a single example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "p_f2wVqN7d-n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "id": "p_f2wVqN7d-n",
    "outputId": "d55125ec-912d-4540-a41f-472cc4805dd6"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Example initial response:\n",
       "\n",
       "Okay, let's solve the equation 3x + 7 = 22:\n",
       "\n",
       "1. **Subtract 7 from both sides:**\n",
       "   3x + 7 - 7 = 22 - 7\n",
       "   3x = 15\n",
       "\n",
       "2. **Divide both sides by 3:**\n",
       "   3x / 3 = 15 / 3\n",
       "   x = 5\n",
       "\n",
       "**Therefore, the solution is x = 5**\n",
       "\n",
       "**To check the solution:**\n",
       "Substitute x=5 back into the original equation:\n",
       "3(5) + 7 = 15 + 7 = 22.  The equation holds true."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Example critique:\n",
       "\n",
       "The response primarily provided a step-by-step solution, which, while correct, didnâ€™t actively engage the student in the problem-solving process. It felt more like presenting the answer than facilitating the studentâ€™s understanding of *how* to arrive at that answer.  It didnâ€™t ask questions to gauge their existing knowledge, nor did it explicitly explain the reasoning behind each step in a way that would help them build confidence in their own abilities to solve similar problems. The \"check the solution\" section was a good addition, but it could have been integrated earlier to reinforce the process."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Example revised response:\n",
       "\n",
       "Letâ€™s solve this equation together and really understand how to get to the answer! The equation is 3x + 7 = 22.\n",
       "\n",
       "1. **Our Goal:**  We want to find the value of â€˜xâ€™ that makes this equation true.  That means we want to isolate â€˜xâ€™ on one side of the equation by itself.\n",
       "\n",
       "2. **Undo Addition/Subtraction First:**  Look closely at the equation.  Right now, we have \"+ 7\" on its own side. To get rid of it, we need to do the opposite operation â€“ subtraction. Letâ€™s subtract 7 from *both* sides of the equation. This keeps the equation balanced.\n",
       "   3x + 7 - 7 = 22 - 7\n",
       "   This simplifies to: 3x = 15\n",
       "\n",
       "3. **Undo Multiplication/Division:** Now we have â€˜3xâ€™. This means 3 is being multiplied by â€˜xâ€™. To isolate â€˜xâ€™, we need to do the opposite â€“ division.  Divide *both* sides of the equation by 3.\n",
       "   3x / 3 = 15 / 3\n",
       "   This simplifies to: x = 5\n",
       "\n",
       "4. **Check Your Work:**  Itâ€™s always a good idea to check if youâ€™ve solved it correctly! Letâ€™s plug x = 5 back into the original equation:\n",
       "   3(5) + 7 = 15 + 7 = 22.  See?  The equation is true!  This means we did it correctly.\n",
       "\n",
       "**Letâ€™s think about why this works:**  Every time we do something to one side of an equation, we *have* to do the same thing to the other side.  This keeps the equation balanced.  We're essentially reversing the operations that were performed on 'x' to isolate it.\n",
       "\n",
       "Do you want to try another similar problem to practice this strategy?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage\n",
    "split, idx, result = create_sample(\n",
    "    \"train\", 0, ds['train'][0]['prompt'],\n",
    "    model, tokenizer, config,\n",
    "    constitutions\n",
    ")\n",
    "\n",
    "# Display results\n",
    "printmd(f'#### Example initial response:\\n\\n{result[\"init_response\"]}')\n",
    "printmd(f'#### Example critique:\\n\\n{result[\"critic_response\"]}')\n",
    "printmd(f'#### Example revised response:\\n\\n{result[\"revision_response\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f2040e-67de-4d7a-a266-b7cf46c6e3c7",
   "metadata": {
    "id": "b3f2040e-67de-4d7a-a266-b7cf46c6e3c7"
   },
   "source": [
    "## Create dataset\n",
    "\n",
    "> Check out the [generate_dataset.py](./generate_dataset.py) script to run the procedure to produce the training, validation, and test sets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ytof6eJSEDtB",
   "metadata": {
    "id": "ytof6eJSEDtB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}